<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Galin&#39;s Blog</title>
  
  <subtitle>有一说一</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ga-lin.site/"/>
  <updated>2020-01-30T16:09:48.669Z</updated>
  <id>https://ga-lin.site/</id>
  
  <author>
    <name>Galin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>逻辑回归与梯度下降法</title>
    <link href="https://ga-lin.site/2020/01/30/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
    <id>https://ga-lin.site/2020/01/30/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</id>
    <published>2020-01-30T15:49:08.000Z</published>
    <updated>2020-01-30T16:09:48.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="逻辑回归与梯度下降法">逻辑回归与梯度下降法</h2><p>在此之前，我们得先重温一下回归，很多时候我们只记得公式，知道逻辑回归是怎么一回事，却忘了回归是什么，怎么来的</p><p>​<strong>回归是一种归纳的思想，在深度学习领域，就是从样本的数据出发，确定某些变量之间的定量关系式；这个建立数学模型并估计未知参数的过程就叫做回归分析</strong></p><p>好了，我们给出关系式：<br>$$<br>ŷ=σ(w^Tx+b)<br>$$</p><p>$$<br>σ(z)=\frac{1}{1+e^{-z}}<br>$$</p><p>这里我们设w和b均为一维</p><p>怎样估计w和b？我们总是希望预测值ŷ尽可能地拟合样本值y，损失函数描述了这种拟合关系，在逻辑回归中，使用交叉熵损失函数：<br>$$<br>L(\hat y,y)=-[ylog\hat y+(1-y)log(1-\hat y)]<br>$$</p><p>我们可以将前面两个关系式代入，得到一个关于w和b的式子，而对于每一个样本都可以得到一个损失函数，设有m个样本，对这些损失函数求和得到一个成本函数：<br>$$<br>J(w,b)=\frac {1}{m} \sum _ {i=1}^m L(\hat y^{(i)}, y^{(i)})<br>$$</p><p>接下来问题变成了如何找到一组w和b，使得这个成本函数尽可能地小，可以形象地将其表示为一个“找碗底”的过程<br><img src="/2020/01/30/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.png" alt="梯度下降法"><br>这就是梯度下降法所要做的，从初始点开始，每一次迭代都使用如下公式更新w和b，使得成本函数总是沿着最接近“碗底”的方向下降<br>$$<br>w=w-α\frac {∂J(w,b)}{∂w}<br>$$</p><p>$$<br>b=b-α\frac {∂J(w,b)}{∂b}<br>$$</p><p>其中α为学习率，用以控制每一次梯度下降的步长</p><p>关于初始化w和b，几乎是任意的初始化方法都有效，因为成本函数是凸函数，无论在哪里初始化，使用梯度下降法都应该能达到”碗底“或其附近，这也是为什么选择交叉熵作为损失函数的原因，它总能得到或接近一个全局最优解</p><p>下面我们只关注w，忽略b，具体来看看梯度下降法为什么能够”找到碗底“<br><img src="/2020/01/30/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/%E6%9B%B4%E5%A5%BD%E7%9A%84%E8%AF%B4%E6%98%8E.png" alt="更好的说明"><br>由上图可以看到，无论怎样初始化w，梯度下降法总是使w朝着成本函数减少的方向移动的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;逻辑回归与梯度下降法&quot;&gt;逻辑回归与梯度下降法&lt;/h2&gt;
&lt;p&gt;在此之前，我们得先重温一下回归，很多时候我们只记得公式，知道逻辑回归是怎么一回事，却忘了回归是什么，怎么来的&lt;/p&gt;
&lt;p&gt;​	&lt;strong&gt;回归是一种归纳的思想，在深度学习领域，就是从样本的数据出发
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://ga-lin.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://ga-lin.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降法" scheme="https://ga-lin.site/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>交叉熵损失函数</title>
    <link href="https://ga-lin.site/2020/01/15/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://ga-lin.site/2020/01/15/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</id>
    <published>2020-01-15T09:53:32.000Z</published>
    <updated>2020-01-23T06:03:29.364Z</updated>
    
    <content type="html"><![CDATA[<p>在理解交叉熵损失函数之前，有必要先来说一下信息熵这个概念</p><h2 id="信息熵">信息熵</h2><p>信息熵可以理解为信息杂乱程度的量化描述：信息越多，概率越均等则信息熵就越大；反之，信息越单一，概率越偏向其中某一个信息，那么熵值就越小</p><p>公式如下：</p><p>$$<br>H(X)=-\sum _ {i=1}^n P(x^{(i)}) logP(x^{(i)})<br>$$</p><p>其中，</p><p>$$<br>-logP(x^{(i)})<br>$$<br>表示一个信息的信息量，概率越小，信息量就越大，这很好理解，比如“太阳从西边出来了”，这几乎不可能发生，如果发生了，那对于我们来说其所含的信息量是巨大的</p><h2 id="交叉熵损失函数">交叉熵损失函数</h2><p>先给出公式：<br>$$<br>L(\hat y,y)=-[ylog\hat y+(1-y)log(1-\hat y)]<br>$$<br>其中ŷ为预测值</p><p>我们来解释一下这个公式为什么能起作用：</p><p>对于损失函数，我们希望它越小越好</p><p>当y=1时，L(ŷ,y)=-log ŷ，则ŷ应尽可能接近于1，才能让损失函数尽可能地小<br>当y=0时，L(ŷ,y)=-(1-y)log(1-ŷ)，则ŷ应尽可能接近于0，才能让损失函数尽可能地小</p><p>在逻辑回归中，预测值是一个概率，它表示与样本的拟合程度，而该公式既很好地表达了这种关系，也满足了损失函数的定义</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在理解交叉熵损失函数之前，有必要先来说一下信息熵这个概念&lt;/p&gt;
&lt;h2 id=&quot;信息熵&quot;&gt;信息熵&lt;/h2&gt;
&lt;p&gt;信息熵可以理解为信息杂乱程度的量化描述：信息越多，概率越均等则信息熵就越大；反之，信息越单一，概率越偏向其中某一个信息，那么熵值就越小&lt;/p&gt;
&lt;p&gt;公式如下
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://ga-lin.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://ga-lin.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="交叉熵" scheme="https://ga-lin.site/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>最优性原理</title>
    <link href="https://ga-lin.site/2019/12/10/%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86/"/>
    <id>https://ga-lin.site/2019/12/10/%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86/</id>
    <published>2019-12-10T02:19:40.000Z</published>
    <updated>2019-12-10T02:33:25.405Z</updated>
    
    <content type="html"><![CDATA[<p>看了很多有关最优性原理的解释，总是感觉不清晰、不透彻，以下是我个人的理解</p><p>我们先给出定义：</p><p><strong>最优性原理对于多阶段决策过程的最优决策序列具有如下性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言，其后各阶段的决策序列必须构成最优策略</strong></p><p>它看上去很讨厌，如同与你玩文字游戏，我们单看冒号前的文字，可知：<br>1）首先它指出，这是一个性质，是最优决策序列的性质<br>2）假设我们已知这个最优决策序列</p><p>好了我们就此打住，不继续往下解读了，我们来看一个直观的例子：<br><img src="/2019/12/10/%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86/%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86.png" alt="最优性原理"><br>假设有地点A、B、C、D、E，火车要从A开往E，求最短路线</p><p>这显然是一个多阶段决策问题，火车每到达一个地点，必须选择下一个开往的地点</p><p>假设我们已知这个问题的最优决策序列，显然它是A-&gt;B-&gt;C-&gt;E<br>请一定记住，我们接下来的讨论都是基于这个假设之上的</p><p>火车到达B后，从B开往E的最短路线是什么？</p><p>我们不知道，我们现在只知道：<br>1）从A开往E的最短路线A-&gt;B-&gt;C-&gt;E<br>2）B处有两个决策：开往C或开往D</p><p>此时，冒号之后的文字起了作用，在这个问题里，它表达为：从B开往E的最短路线必须在A-&gt;B-&gt;C-&gt;E上，即B-&gt;C-&gt;E，故火车在B处选择开往C</p><p>换一个角度理解，那么最优性原理想要表达的就是：<br><strong>对于多阶段决策问题，整个问题的最优决策序列，一定包含了它子问题的最优决策序列<br>不严谨地说就是，多阶段决策问题的整体最优解一定能使得局部最优</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;看了很多有关最优性原理的解释，总是感觉不清晰、不透彻，以下是我个人的理解&lt;/p&gt;
&lt;p&gt;我们先给出定义：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最优性原理	对于多阶段决策过程的最优决策序列具有如下性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言，其后各阶段的决策序
      
    
    </summary>
    
    
      <category term="算法" scheme="https://ga-lin.site/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="动态规划" scheme="https://ga-lin.site/categories/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
    
      <category term="算法" scheme="https://ga-lin.site/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="动态规划" scheme="https://ga-lin.site/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
      <category term="最优性原理" scheme="https://ga-lin.site/tags/%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>斜投影——学习笔记</title>
    <link href="https://ga-lin.site/2019/11/20/%E6%96%9C%E6%8A%95%E5%BD%B1%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://ga-lin.site/2019/11/20/%E6%96%9C%E6%8A%95%E5%BD%B1%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2019-11-20T00:20:07.000Z</published>
    <updated>2019-11-27T15:41:07.273Z</updated>
    
    <content type="html"><![CDATA[<p>当投射直线与投影面不垂直时，投影面得到的就是斜投影，下面我们用一个直观的例子来说明</p><h2 id="一个直观的例子："><strong>一个直观的例子：</strong></h2><p>假设：<br>1）投影平面为 z=0<br>2）直角坐标为(0，0，1)的点P通过斜投影得到点P’<br>3）L为P’到坐标原点的距离<br>4）α为OP’与x轴正向所形成的角<br>5）β为投射直线与投影平面所成的角<br><img src="/2019/11/20/%E6%96%9C%E6%8A%95%E5%BD%B1%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%96%9C%E6%8A%95%E5%BD%B1.png" alt="斜投影"></p><p>设三维空间中有直角坐标为(x，y，z)的任意一点Q’（如立方体的顶点），通过斜投影所得投影点的直角坐标为Q’（x’，y’，z’），显然 z’=0，则<br><img src="/2019/11/20/%E6%96%9C%E6%8A%95%E5%BD%B1%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%B2%BFy%E8%BD%B4%E8%B4%9F%E6%96%B9%E5%90%91.png" alt="沿y轴负方向"><br>沿y轴负方向看去（如上图），可得：<br>$$<br>\frac{x’-x}{z}=\frac{Lcosα}{1}<br>$$</p><p>同理，沿x轴负方向看去可得：<br>$$<br>\frac{y’-y}{z}=\frac{Lsinα}{1}<br>$$</p><p>因此有斜投影变换公式：<br>$$<br>x’=x+z(Lcosα),y’=y+z(Lsinα)<br>$$<br>可将三维空间中任意一点斜投影至平面 z=0上</p><h2 id="如何得到斜二测投影？"><strong>如何得到斜二测投影？</strong></h2><p>实际上，观察图 1可知，投影平面上斜投影点的位置与角β有关，而角β受到点P位置和L大小的控制<br>我们可以调整点P位置和L大小，使得</p><p>当tanβ=2时，即可得到斜二测投影（斜二测投影使垂直于投影面的线段长度缩短为原来的一半）<br>当tanβ=1时，得到斜等测投影（斜等测投影使垂直于投影面的线段仍保持长度）</p><p>这时的α角还可以不同</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当投射直线与投影面不垂直时，投影面得到的就是斜投影，下面我们用一个直观的例子来说明&lt;/p&gt;
&lt;h2 id=&quot;一个直观的例子：&quot;&gt;&lt;strong&gt;一个直观的例子：&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;假设：&lt;br&gt;
1）投影平面为 z=0&lt;br&gt;
2）直角坐标为(0，0，1)的
      
    
    </summary>
    
    
      <category term="计算机图形学" scheme="https://ga-lin.site/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
      <category term="计算机图形学" scheme="https://ga-lin.site/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
      <category term="学习笔记" scheme="https://ga-lin.site/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Welcome!</title>
    <link href="https://ga-lin.site/2019/11/11/Welcome/"/>
    <id>https://ga-lin.site/2019/11/11/Welcome/</id>
    <published>2019-11-11T11:04:17.000Z</published>
    <updated>2020-01-15T09:51:25.205Z</updated>
    
    <content type="html"><![CDATA[<p>非常高兴能够搭建自己的个人博客<br>​感谢 <a href="https://www.codesheep.cn/" target="_blank" rel="noopener">CodeSheep</a> 大大的视频 <a href="https://www.bilibili.com/video/av44544186" target="_blank" rel="noopener">手把手教你从0开始搭建自己的个人博客 |无坑版视频教程| hexo</a></p><p>今后将在这里记录下自己的Coding旅程🙈</p><p>Welcome!<br><img src="/2019/11/11/Welcome/Welcome.png" alt="Welcome"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;非常高兴能够搭建自己的个人博客&lt;br&gt;
​感谢 &lt;a href=&quot;https://www.codesheep.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CodeSheep&lt;/a&gt; 大大的视频 &lt;a href=&quot;https://www.bilib
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
